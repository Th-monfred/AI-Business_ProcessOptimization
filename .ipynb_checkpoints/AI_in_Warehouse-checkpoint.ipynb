{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the flows in an e-commerce warehouse (Example)\n",
    "\n",
    "## 1) The case study\n",
    "\n",
    "## 2) AI Solution\n",
    "\n",
    "## 3) Going into production\n",
    "\n",
    "****************************\n",
    "\n",
    "## 1) The case study\n",
    "\n",
    "The problem to solve is to optimize the flows inside a warehouse, where the products are stored in 12 different locations, labeled by letters from A to L:\n",
    "\n",
    "![alt text](maps.png \"Warehouse map\")\n",
    "\n",
    "By costumer online demand, an autonomous warehouse robot collects the products in the warehouse for future delivering. \n",
    "\n",
    "![alt text](robot.png \"Warehouse Robot\")\n",
    "\n",
    "Priority order must be followed to transport products safely\n",
    "\n",
    "![alt text](priorities.png \"Priorities\")\n",
    "\n",
    "\n",
    "To solve the problem, we'll build an AI that will always take the shortest route to the top priority location, whatever the location it starts from. We'll apply an AI with reinforcement learning (Q-learning).\n",
    "\n",
    "In this case, we import numpy for Python. Also we define two key parameters related to Q-learning, alpha and gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining environment\n",
    "\n",
    "+ Defining states\n",
    "\n",
    "+ Defining actions\n",
    "\n",
    "+ Defining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining states:\n",
    "\n",
    "# python dictionary. Local states\n",
    "\n",
    "location_to_state = {'A': 0,\n",
    "                     'B': 1,\n",
    "                     'C': 2,\n",
    "                     'D': 3,\n",
    "                     'E': 4,\n",
    "                     'F': 5,\n",
    "                     'G': 6,\n",
    "                     'H': 7,\n",
    "                     'I': 8,\n",
    "                     'J': 9,\n",
    "                     'K': 10,\n",
    "                     'L': 11}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the local states, we can define the actions. Let say the robot is in location F (see the map). It has only two options, go to B (1) or J (9). The total list of actions the AI can play overall is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining actions:\n",
    "\n",
    "actions = [0,1,2,3,4,5,6,7,8,9,10,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to define a reward function R that takes as input a state $s$ and an action $a$, and returns a numerical reward that the AI will get by playing the action $a$ in the state $s$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R: (s, a) $\\mapsto$ r $\\in$ $\\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a discrete and finite number of states and actions, we'll build our reward function R making a matrix. Our reward function will exactly be a matrix of 12 rows and 12 columns, where the rows correspond to the states, and the columns correspond to the actions. Also we can attribute a 0 reward to actions that the robot cannot play and 1 to actions the robot can play. For example, if the robot starts at the location A, its only option is to go to B, which receives reward 1.\n",
    "\n",
    "![alt text](rewards1.png \"Reward A\")\n",
    "\n",
    "Based in the warehouse map we have, the complete matrix should be:\n",
    "\n",
    "![alt text](rewards2.png \"All Rewards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining rewards:\n",
    "\n",
    "R = np.array([\n",
    "        [0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "        [1,0,1,0,0,1,0,0,0,0,0,0],\n",
    "        [0,1,0,0,0,0,1,0,0,0,0,0],\n",
    "        [0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "        [0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "        [0,1,0,0,0,0,0,0,0,1,0,0],\n",
    "        [0,0,1,0,0,0,1000,1,0,0,0,0],\n",
    "        [0,0,0,1,0,0,1,0,0,0,0,1],\n",
    "        [0,0,0,0,1,0,0,0,0,1,0,0],\n",
    "        [0,0,0,0,0,1,0,0,1,0,1,0],\n",
    "        [0,0,0,0,0,0,0,0,0,1,0,1],\n",
    "        [0,0,0,0,0,0,0,1,0,0,1,0],\n",
    "       ])\n",
    "\n",
    "## As G is the top priority location, we manually put a high reward in the position cell correspondent to (G,G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) AI Solution\n",
    "\n",
    "The AI Solution to solve the problem is a Q-Learning model, which is based on Markov Decision Processes (MDP). \n",
    "\n",
    "### Markov Decision Processes\n",
    "\n",
    "A Markov Decision Process is a tuple (S, A, T, R) where:\n",
    "\n",
    "+ S is the set of the different states.\n",
    "\n",
    "S = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
    "\n",
    "+ A is the set of the different actions that can be played at each time t.\n",
    "\n",
    "A = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
    "\n",
    "+ T is called the transition rule:\n",
    "\n",
    "T: ($s_t$  S, $s_{t+1}$ $\\in$ S, $a_t$ $\\in$ A) $\\mapsto$ $\\mathbb{P}$( $s_{t+1}$ | $s_t$, $a_t$)\n",
    "\n",
    "where $\\mathbb{P}$ ( $s_{t+1}$ | $s_t$, $a_t$) is the probability to reach the future state $s_{t+1}$ when playing the action $a_t$ in the state $s_t$. Therefore T is the probability distribution of the future states at time t + 1 given the current state and the action played at time t. Accordingly, we can predict the future state $s_(t+1)$ by taking a random draw from that distribution T:\n",
    "\n",
    "$s_{t+1}$ $\\times$ T($s_t , ., a_t$)\n",
    "\n",
    "+ R is the reward function:\n",
    "\n",
    "R: ($s_t$ $\\in$ S, $a_t$ $\\in$ A) $\\mapsto$ $r_t$ $\\in$ $\\mathbb{R}$\n",
    "\n",
    "where $r_t$ is the reward obtained after playing the action $a_t$ in the state $s_t$ .\n",
    "\n",
    "The probability of the future step (state $s_{t+1}$) depends just and only on the current state and action, $s_t$ and $a_t$ respectively, and not on the previous steps. Markov Decision Process has no memory. In math language:\n",
    "\n",
    "$\\mathbb{P}$($s_{t+1}$|$s_0$, $a_0$, $s_1$, $a_1$, $s_2$, $a_3$, ..., $s_t$, $a_t$) = $\\mathbb{P}$($s_{t+1}$|$s_t$, $a_t$).\n",
    "\n",
    "To recap:\n",
    "\n",
    "1 - The AI observes its current state $s_t$\\\n",
    "2 - The AI plays the action $a_{t}$\\\n",
    "3 - The AI receives the reward $r_t$ = R($s_t$, $a_t$)\\\n",
    "4 - The AI enters the following state $s_{t+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the AI solution with Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "### Q-Value\n",
    "\n",
    "To each couple of state and action $(s, a)$, we associate a numeric value Q$(s, a)$:\n",
    "\n",
    "Q: ($s$ $\\in$ $S$, $a$ $\\in$ $A$) 7 $\\mapsto$ Q$(s, a)$ $\\in$ $\\mathbb{R}$\n",
    "\n",
    "We may say that Q$(s, a)$ is \"the Q-value of the action a played in the state s\".\n",
    "\n",
    "### The Temporal Difference\n",
    "\n",
    "At the beginning t = 0, all the Q-values are initialized to 0:\n",
    "\n",
    "$\\forall$$s$ $\\in$ S, $a \\in A$, Q$(s,a)$ = 0\n",
    "\n",
    "The temporal difference at time t, denoted by TD$_t(s_t, a_t)$, is the difference between:\n",
    "\n",
    "+ R$(s_t , a_t )$ + $\\gamma$max(Q($s_{t+1}, a$)), that is the reward R($s_t , a_t$) obtained by playing the action $a_t$ in the state $s_t$ , plus the Q-Value of the best action played in the future state $s_{t+1}$, discounted by a factor $\\gamma \\in$ [0, 1], called the discount factor.\n",
    "\n",
    "and\n",
    "\n",
    "+ Q($s_t , a_t$), that is the Q-Value of the action $a_t$ played in the state $s_t$,\n",
    "\n",
    "so that:\n",
    "\n",
    "$TD_t$($s_t , a_t$) = R($s_t , a_t$) + $\\gamma$max(Q($s_{t+1} , a$)) − Q($s_t , a_t$)\n",
    "\n",
    "+ If $TD_t(s_t, a_t)$ is high, the AI gets a \"good surprise\"\n",
    "\n",
    "+ If $TD_t(s_t, a_t)$ is high, the AI gets a \"frustration\".\n",
    "\n",
    "AI will interate some updates of the Q-values (through Bellman equation) towards higher temporal differences. In the final step, the TD reinforce the couple (state, action) from t-1 to time t, according to:\n",
    "\n",
    "$Q_t$($s_t, a_t$) = Q_${t−1}$($s_t, a_t$) + $\\alpha$$TD_t$($s_t, a_t$),\n",
    "\n",
    "where $\\alpha \\in \\mathbb{R}$ is the learning rate.\n",
    "\n",
    "## The whole Q-Learning algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.75\n",
    "alpha = 0.9\n",
    "\n",
    "# Initialization of Q-values:\n",
    "\n",
    "Q = np.array(np.zeros([12,12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Selecting a random state s_t from our 12 possible states\n",
    "\n",
    "for i in range(1000):\n",
    "    current_state = np.random.randint(0,12)\n",
    "#2) Playing a random action at that can lead to a next possible state, i.e. such R(st, at) > 0\n",
    "\n",
    "playable_actions = []\n",
    "for j in range(12):\n",
    "    if R[current_state, j] > 0:\n",
    "        playable_actions.append(j)\n",
    "next_state = np.random.choice(playable_actions)\n",
    "\n",
    "#3) We reach the next state st+1 and we get the reward R(st, at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) We compute the Temporal Difference TDt(st,at):    \n",
    "\n",
    "TD = R[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state, ])] - Q[current_state, next_state]    \n",
    "\n",
    "#5) We update the Q-value by applying the Bellman equation:    \n",
    "\n",
    "Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Going into production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a mapping from the states to the locations\n",
    "\n",
    "state_to_location = {state: location for location, state in location_to_state.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the final function that will return the optimal route\n",
    "\n",
    "def route(starting_location, ending_location):\n",
    "    route = [starting_location]\n",
    "    next_location = starting_location\n",
    "    while (next_location != ending_location):\n",
    "        starting_state = location_to_state[starting_location]\n",
    "        next_state = np.argmax(Q[starting_state])\n",
    "        next_location = state_to_location[next_state]\n",
    "        route.append(next_location)\n",
    "        starting_location = next_location\n",
    "    return route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the final route\n",
    "\n",
    "print('Route: ')\n",
    "route('F','G')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
